{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from fractions import Fraction\n",
    "import re\n",
    "\n",
    "# Fungsi-fungsi untuk preprocessing\n",
    "def case_folding(text):\n",
    "    return text.lower()\n",
    "\n",
    "def cleansing(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Hanya biarkan huruf dan spasi\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Menghapus spasi ganda\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def pad_sequences(sequences, max_length):\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padded_seq = seq + [0] * (max_length - len(seq))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_seqs.append(padded_seq)\n",
    "    return padded_seqs\n",
    "\n",
    "def create_word_embedding(tokenized_texts):\n",
    "    vocab = set(word for text in tokenized_texts for word in text)\n",
    "    word_to_index = {word: idx + 1 for idx, word in enumerate(vocab)}  # Index 0 untuk padding\n",
    "    word_to_index['<EOS>'] = len(word_to_index) + 1  # Tambahkan <EOS> ke kamus\n",
    "    index_to_word = {idx + 1: word for idx, word in enumerate(vocab)}\n",
    "    index_to_word[len(index_to_word) + 1] = '<EOS>'\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "def encode_text(tokenized_text, word_to_index):\n",
    "    encoded_text = [word_to_index[word] for word in tokenized_text if word in word_to_index]\n",
    "    if '<EOS>' in word_to_index:\n",
    "        encoded_text.append(word_to_index['<EOS>'])\n",
    "    return encoded_text\n",
    "\n",
    "# Load data from Excel\n",
    "def load_data(file_path):\n",
    "    print(\"Loading data from Excel...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df['SOURCE'].tolist(), df['TARGET'].tolist()\n",
    "\n",
    "# LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Fungsi untuk melatih model\n",
    "def train_model(model, optimizer, criterion, source_seqs, target_seqs, word_to_index, index_to_word, max_length, epochs=25, batch_size=2):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Shuffle data setiap epoch\n",
    "        permutation = np.random.permutation(len(source_seqs))\n",
    "        source_seqs = [source_seqs[i] for i in permutation]\n",
    "        target_seqs = [target_seqs[i] for i in permutation]\n",
    "\n",
    "        for i in range(0, len(source_seqs), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_sources = source_seqs[i:i + batch_size]\n",
    "            batch_targets = target_seqs[i:i + batch_size]\n",
    "\n",
    "            padded_sources = pad_sequences(batch_sources, max_length=max_length)\n",
    "            padded_targets = pad_sequences(batch_targets, max_length=max_length)\n",
    "\n",
    "            source_tensor = torch.tensor(padded_sources)\n",
    "            target_tensor = torch.tensor(padded_targets)\n",
    "\n",
    "            output = model(source_tensor)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target_tensor = target_tensor.view(-1)\n",
    "\n",
    "            loss = criterion(output, target_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Tampilkan beberapa contoh prediksi\n",
    "            if i % (batch_size * 10) == 0:  # Setiap 10 batch\n",
    "                print(f\"Batch {i // batch_size}/{len(source_seqs) // batch_size}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "                # Contoh pengujian prediksi pada beberapa data\n",
    "                for j in range(min(5, len(batch_sources))):\n",
    "                    source_text = ' '.join([index_to_word.get(idx, '') for idx in batch_sources[j] if idx != 0])\n",
    "                    target_text = ' '.join([index_to_word.get(idx, '') for idx in batch_targets[j] if idx != 0])\n",
    "                    predicted_text = predict(model, source_text, word_to_index, index_to_word, max_length=max_length)\n",
    "\n",
    "                    print(f\"SOURCE: {source_text}\")\n",
    "                    print(f\"TARGET: {target_text}\")\n",
    "                    print(f\"PREDICTION: {predicted_text}\")\n",
    "                    print()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(source_seqs):.4f}\")\n",
    "\n",
    "# Fungsi untuk melakukan prediksi\n",
    "def predict(model, source_text, word_to_index, index_to_word, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokenized_source = tokenize(cleansing(case_folding(source_text)))\n",
    "        encoded_source = encode_text(tokenized_source, word_to_index)\n",
    "        padded_source = pad_sequences([encoded_source], max_length=max_length)[0]\n",
    "        source_tensor = torch.tensor(padded_source).unsqueeze(0)\n",
    "\n",
    "        output = model(source_tensor)\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        predicted_words = []\n",
    "        eos_token = word_to_index.get('<EOS>', None)\n",
    "        predicted_indices = []\n",
    "\n",
    "        for timestep in range(output.size(0)):\n",
    "            token_idx = torch.argmax(output[timestep]).item()\n",
    "            if token_idx == 0:  # Padding token, stop further decoding\n",
    "                break\n",
    "            if eos_token and token_idx == eos_token:  # EOS token, stop decoding\n",
    "                break\n",
    "            if len(predicted_indices) > 0 and token_idx == predicted_indices[-1]:  # Skip repeating tokens\n",
    "                continue\n",
    "            predicted_words.append(index_to_word.get(token_idx, ''))\n",
    "            predicted_indices.append(token_idx)\n",
    "\n",
    "        predicted_text = ' '.join(predicted_words).strip()\n",
    "        return predicted_text\n",
    "\n",
    "# Fungsi untuk menghitung BLEU score secara manual\n",
    "def calculate_bleu_score(references, hypotheses, max_n=4):\n",
    "    p_numerators = Counter()\n",
    "    p_denominators = Counter()\n",
    "    reference_lengths = Counter()\n",
    "    hypothesis_lengths = Counter()\n",
    "\n",
    "    for reference, hypothesis in zip(references, hypotheses):\n",
    "        reference_lengths[len(reference[0])] += 1\n",
    "        hypothesis_lengths[len(hypothesis)] += 1\n",
    "\n",
    "        for i in range(1, max_n + 1):\n",
    "            p_i = modified_precision(reference[0], hypothesis, i)\n",
    "            p_numerators[i] += p_i.numerator\n",
    "            p_denominators[i] += p_i.denominator\n",
    "\n",
    "    p_n = [p_numerators[i] / p_denominators[i] if p_denominators[i] > 0 else 0 for i in range(1, max_n + 1)]\n",
    "    bp = brevity_penalty(reference_lengths, hypothesis_lengths)\n",
    "    bleu = bp * np.exp(sum(np.log(p) for p in p_n if p > 0) / max_n)\n",
    "    return bleu\n",
    "\n",
    "# Fungsi untuk menghitung modified precision\n",
    "def modified_precision(reference, hypothesis, n):\n",
    "    reference_ngrams = Counter(tuple(reference[i:i + n]) for i in range(len(reference) - n + 1))\n",
    "    hypothesis_ngrams = Counter(tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) - n + 1))\n",
    "\n",
    "    clipped_counts = dict()\n",
    "    for ngram in hypothesis_ngrams:\n",
    "        clipped_counts[ngram] = min(hypothesis_ngrams[ngram], reference_ngrams[ngram])\n",
    "\n",
    "    numerator = sum(clipped_counts.values())\n",
    "    denominator = max(1, sum(hypothesis_ngrams.values()))\n",
    "\n",
    "    return Fraction(numerator, denominator)\n",
    "\n",
    "# Fungsi untuk menghitung Brevity Penalty\n",
    "def brevity_penalty(reference_lengths, hypothesis_lengths):\n",
    "    c = sum(reference_lengths.values())\n",
    "    r = sum(hypothesis_lengths.values())\n",
    "    if r > c:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.exp(1 - c / r)\n",
    "\n",
    "# Fungsi utama untuk melatih dan menguji model NMT\n",
    "def main(file_path):\n",
    "    source_texts, target_texts = load_data(file_path)\n",
    "\n",
    "    # Preprocessing data\n",
    "    source_texts = [cleansing(case_folding(text)) for text in source_texts]\n",
    "    target_texts = [cleansing(case_folding(text)) for text in target_texts]\n",
    "\n",
    "    max_length = 350  # Tentukan panjang maksimum untuk padding\n",
    "\n",
    "    source_tokenized = [tokenize(text) for text in source_texts]\n",
    "    target_tokenized = [tokenize(text) for text in target_texts]\n",
    "\n",
    "    # Membuat word embedding\n",
    "    word_to_index, index_to_word = create_word_embedding(source_tokenized + target_tokenized)\n",
    "\n",
    "    # Encoding teks ke dalam index dan padding sequence\n",
    "    source_seqs = [encode_text(tokenized_text, word_to_index) for tokenized_text in source_tokenized]\n",
    "    target_seqs = [encode_text(tokenized_text, word_to_index) for tokenized_text in target_tokenized]\n",
    "\n",
    "    # Membagi data menjadi data latih dan data uji (85% latih, 15% uji)\n",
    "    source_train, source_test, target_train, target_test = train_test_split(source_seqs, target_seqs, test_size=0.15, random_state=42)\n",
    "\n",
    "    vocab_size = len(word_to_index) + 1  # Ditambah 1 untuk padding token\n",
    "    embedding_dim = 128\n",
    "    hidden_dim = 256\n",
    "\n",
    "    model = LSTM(vocab_size, embedding_dim, hidden_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignorasi index 0 (padding)\n",
    "\n",
    "    # Melatih model\n",
    "    train_model(model, optimizer, criterion, source_train, target_train, word_to_index, index_to_word, max_length=max_length, epochs=25)\n",
    "\n",
    "    # Evaluasi model menggunakan BLEU score pada data uji\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    exact_matches = 0\n",
    "\n",
    "    # Tampilkan kalimat-kalimat yang dijadikan data testing\n",
    "    print(\"Kalimat yang dijadikan data testing:\")\n",
    "    for i in range(len(source_test)):\n",
    "        source_text = ' '.join([index_to_word.get(idx, '') for idx in source_test[i] if idx != 0])\n",
    "        target_text = ' '.join([index_to_word.get(idx, '') for idx in target_test[i] if idx != 0])\n",
    "        print(f\"SOURCE: {source_text}\")\n",
    "        print(f\"TARGET: {target_text}\")\n",
    "        print()\n",
    "\n",
    "    print(\"\\nProses prediksi dan evaluasi:\")\n",
    "    for i in range(len(source_test)):\n",
    "        source_text = ' '.join([index_to_word.get(idx, '') for idx in source_test[i] if idx != 0])\n",
    "        target_text = ' '.join([index_to_word.get(idx, '') for idx in target_test[i] if idx != 0])\n",
    "        predicted_text = predict(model, source_text, word_to_index, index_to_word, max_length=max_length)\n",
    "\n",
    "        references.append([target_text.split()])\n",
    "        hypotheses.append(predicted_text.split())\n",
    "\n",
    "        if predicted_text == target_text:\n",
    "            exact_matches += 1\n",
    "\n",
    "        print(f\"SOURCE: {source_text}\")\n",
    "        print(f\"TARGET: {target_text}\")\n",
    "        print(f\"PREDICTION: {predicted_text}\")\n",
    "        print()\n",
    "\n",
    "    bleu_score = calculate_bleu_score(references, hypotheses)\n",
    "    exact_match_ratio = exact_matches / len(source_test)\n",
    "\n",
    "    # Testing setelah evaluasi\n",
    "    print(\"\\nProses Testing:\")\n",
    "    for i in range(len(source_test)):\n",
    "        source_text = ' '.join([index_to_word.get(idx, '') for idx in source_test[i] if idx != 0])\n",
    "        target_text = ' '.join([index_to_word.get(idx, '') for idx in target_test[i] if idx != 0])\n",
    "        predicted_text = predict(model, source_text, word_to_index, index_to_word, max_length=max_length)\n",
    "\n",
    "        print(f\"SOURCE: {source_text}\")\n",
    "        print(f\"TARGET: {target_text}\")\n",
    "        print(f\"PREDICTION: {predicted_text}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    print(f\"\\nHasil Evaluasi:\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"Exact Match Ratio: {exact_match_ratio:.4f}\")\n",
    "\n",
    "    # Simpan model\n",
    "    with open(\"muna_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model saved as muna_model.pkl\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(\"Corpus_muna.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
