{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membaca file Excel\n",
    "def read_excel(file_name):\n",
    "    wb = openpyxl.load_workbook(file_name)\n",
    "    sheet = wb.active\n",
    "    source_data = []\n",
    "    target_data = []\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        source_data.append(row[0])\n",
    "        target_data.append(row[1])\n",
    "    return source_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk case folding (mengubah ke huruf kecil)\n",
    "def case_folding(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk cleansing (menghapus tanda baca)\n",
    "def cleansing(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk tokenisasi\n",
    "def tokenization(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menambahkan padding pada token\n",
    "def add_padding(tokens, max_length):\n",
    "    return tokens + ['<PAD>'] * (max_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk embedding kata (mengubah kata menjadi vektor numerik)\n",
    "def word_embedding(tokens, vocab):\n",
    "    return [vocab[word] if word in vocab else vocab['<UNK>'] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membangun kosakata (vocab) dari data\n",
    "def build_vocab(data):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "    index = 4\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk memproses data teks\n",
    "def preprocess(file_name):\n",
    "    source_data, target_data = read_excel(file_name)\n",
    "    processed_source_data = []\n",
    "    processed_target_data = []\n",
    "\n",
    "    for source_text, target_text in zip(source_data, target_data):\n",
    "        source_text = case_folding(source_text)\n",
    "        source_text = cleansing(source_text)\n",
    "        source_tokens = tokenization(source_text)\n",
    "        source_tokens = ['<SOS>'] + source_tokens + ['<EOS>']\n",
    "        processed_source_data.append(source_tokens)\n",
    "\n",
    "        target_text = case_folding(target_text)\n",
    "        target_text = cleansing(target_text)\n",
    "        target_tokens = tokenization(target_text)\n",
    "        target_tokens = ['<SOS>'] + target_tokens + ['<EOS>']\n",
    "        processed_target_data.append(target_tokens)\n",
    "\n",
    "    return processed_source_data, processed_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelas Dataset untuk NMT\n",
    "class NMTCorpusDataset(Dataset):\n",
    "    def __init__(self, source_data, target_data, vocab):\n",
    "        self.source_data = source_data\n",
    "        self.target_data = target_data\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.max_length = max(max(len(sentence) for sentence in source_data), max(len(sentence) for sentence in target_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_tokens = self.source_data[idx]\n",
    "        target_tokens = self.target_data[idx]\n",
    "\n",
    "        padded_source_tokens = add_padding(source_tokens, self.max_length)\n",
    "        padded_target_tokens = add_padding(target_tokens, self.max_length)\n",
    "\n",
    "        embedded_source_tokens = torch.tensor(word_embedding(padded_source_tokens, self.vocab))\n",
    "        embedded_target_tokens = torch.tensor(word_embedding(padded_target_tokens, self.vocab))\n",
    "\n",
    "        return embedded_source_tokens, embedded_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelas Encoder dengan LSTM di PyTorch\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelas Decoder dengan LSTM di PyTorch\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, target_seq, hidden, cell):\n",
    "        embedded = self.embedding(target_seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        output = self.output_layer(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membangun model NMT\n",
    "def build_NMT(input_dim, hidden_dim, vocab_size):\n",
    "    encoder = Encoder(input_dim, hidden_dim, vocab_size)\n",
    "    decoder = Decoder(input_dim, hidden_dim, vocab_size)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk melatih model NMT\n",
    "def train_NMT(encoder, decoder, train_loader, epochs, learning_rate, vocab):\n",
    "    criterion = nn.NLLLoss(ignore_index=vocab['<PAD>'])\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "    clip = 1.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_seq, target_seq = batch\n",
    "            input_seq = input_seq.to(torch.int64)  # Convert to long\n",
    "            target_seq = target_seq.to(torch.int64)  # Convert to long\n",
    "\n",
    "            hidden, cell = encoder(input_seq)\n",
    "            output, _, _ = decoder(target_seq, hidden, cell)\n",
    "            \n",
    "            loss = criterion(output.permute(0, 2, 1), target_seq)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_loss}')\n",
    "\n",
    "        # Tampilkan beberapa contoh kalimat dan hasil prediksi\n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            for i in range(min(5, len(batch[0]))):\n",
    "                input_seq = batch[0][i, :-1].unsqueeze(0)\n",
    "                target_seq = batch[1][i, 1:].unsqueeze(0)\n",
    "                hidden, cell = encoder(input_seq)\n",
    "                output, _, _ = decoder(target_seq, hidden, cell)\n",
    "                pred_tokens = output.argmax(2).squeeze().tolist()\n",
    "\n",
    "                source_text = [key for key, value in vocab.items() if value in input_seq.squeeze().tolist()]\n",
    "                target_text = [key for key, value in vocab.items() if value in target_seq.squeeze().tolist()]\n",
    "                predicted_text = [key for key, value in vocab.items() if value in pred_tokens]\n",
    "\n",
    "                print(f'Source: {\" \".join(source_text)}')\n",
    "                print(f'Target: {\" \".join(target_text)}')\n",
    "                print(f'Predicted: {\" \".join(predicted_text)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk memprediksi hasil dari model\n",
    "def predict(encoder, decoder, dataset, vocab, max_length):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source_tokens, _ in dataset:\n",
    "            source_tokens = source_tokens.unsqueeze(0)  # Tambahkan batch dimension\n",
    "            hidden, cell = encoder(source_tokens)\n",
    "\n",
    "            # Inisialisasi token awal untuk decoder\n",
    "            token = torch.tensor([[vocab['<SOS>']]])\n",
    "\n",
    "            # Membuat list untuk menampung prediksi\n",
    "            predicted_sentence = []\n",
    "\n",
    "            # Melakukan prediksi satu per satu token\n",
    "            for _ in range(max_length):\n",
    "                output, hidden, cell = decoder(token, hidden, cell)\n",
    "                topv, topi = output.topk(1)\n",
    "                token = topi.squeeze().detach()\n",
    "                predicted_sentence.append(token.item())\n",
    "                if token.item() == vocab['<EOS>']:\n",
    "                    break\n",
    "\n",
    "            predictions.append(predicted_sentence)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung BLEU score\n",
    "def calculate_bleu_score(predictions, references, vocab):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_scores = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = [key for key, value in vocab.items() if value in pred]\n",
    "        ref_tokens = [[key for key, value in vocab.items() if value in ref]]\n",
    "        bleu = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothie)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    avg_bleu_score = np.mean(bleu_scores)\n",
    "    return avg_bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 6.597913646697998\n",
      "Source: <PAD> <SOS> <EOS> saya kamu mengapa merobek buku\n",
      "Target: <PAD> <EOS> noafa obhinighoo bokuku\n",
      "Predicted: <SOS> <EOS>\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> belum yang sudah dan dua tiga ekor pernah beranak ayamnya\n",
      "Target: <PAD> <EOS> raa tolu manuno ghulu bhangkele moose\n",
      "Predicted: <SOS> <EOS>\n",
      "\n",
      "Epoch 2/10, Train Loss: 4.917537460327148\n",
      "Source: <PAD> <SOS> <EOS> dia dibelikan sarung\n",
      "Target: <PAD> <EOS> degholiane bheta\n",
      "Predicted: <SOS> <EOS> o\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> kalau jagung patah tongkol mudah dipatahkan\n",
      "Target: <PAD> <EOS> ane kahitela wurino nopobhera dobherae\n",
      "Predicted: <SOS> <EOS> o\n",
      "\n",
      "Epoch 3/10, Train Loss: 4.433584880828858\n",
      "Source: <PAD> <SOS> <EOS> anak panah\n",
      "Target: <PAD> <EOS> anano pana\n",
      "Predicted: <SOS> <EOS> o\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> kami pekerjaan upah bagibagi\n",
      "Target: <PAD> <EOS> mani tambono karadhaa tabagebagee\n",
      "Predicted: <SOS> <EOS> o miina bhe\n",
      "\n",
      "Epoch 4/10, Train Loss: 4.035468187332153\n",
      "Source: <PAD> <SOS> <EOS> saya orang dari tua kebun juga sebagian diberikan\n",
      "Target: <PAD> <EOS> dua kanau kamokula galuno dowaa sebhila\n",
      "Predicted: <SOS> <EOS> dua o kanau bhe ne\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> dia saja pulang akhirnya\n",
      "Target: <PAD> <EOS> ahirino tanosulimo\n",
      "Predicted: <SOS> <EOS> ane rampano te\n",
      "\n",
      "Epoch 5/10, Train Loss: 3.687225036621094\n",
      "Source: <PAD> <SOS> <EOS> yang besok kami pinangan membawa menunggu rombongan\n",
      "Target: <PAD> <EOS> so naewine kafeena taeantagi meowano\n",
      "Predicted: <SOS> <EOS> so te\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> sakit karena menangis anak\n",
      "Target: <PAD> <EOS> o anahi rampahano nosaki nokobharo\n",
      "Predicted: <SOS> <EOS> so anahi rampahano\n",
      "\n",
      "Epoch 6/10, Train Loss: 3.3623021697998046\n",
      "Source: <PAD> <SOS> <EOS> anak kecil\n",
      "Target: <PAD> <EOS> anahi\n",
      "Predicted: <SOS> <EOS> rampano miina\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> saya berat dua badan puluh enam kilo\n",
      "Target: <PAD> <EOS> kilo raa fulu nomo kabhieku\n",
      "Predicted: <SOS> <EOS> o raa kaeta fulu\n",
      "\n",
      "Epoch 7/10, Train Loss: 3.0732899284362794\n",
      "Source: <PAD> <SOS> <EOS> dia dari jauh akibat semua negeri penyakit keberkatan\n",
      "Target: <PAD> <EOS> rampano saki nokodoho liwu kabarakatino\n",
      "Predicted: <SOS> <EOS> rampano mbali liwu\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> mereka perahu habis dilubangi tikus\n",
      "Target: <PAD> <EOS> nopadae bhangkando nobhentalie wulawo\n",
      "Predicted: <SOS> <EOS> rampano mbali\n",
      "\n",
      "Epoch 8/10, Train Loss: 2.8013722133636474\n",
      "Source: <PAD> <SOS> <EOS> saya kalau tidak apaapa ada anda berbuat dapat tak\n",
      "Target: <PAD> <EOS> ane inodi pae bhae sintu aeafamo\n",
      "Predicted: <SOS> <EOS> desa so miina na inodi pae\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> dia pergi ke di atas supaya naik lempar layanglayangku langit\n",
      "Target: <PAD> <EOS> kala te bhatende kaghatiku nofonighoo lani\n",
      "Predicted: <SOS> <EOS> desa te kaeta\n",
      "\n",
      "Epoch 9/10, Train Loss: 2.5518498325347903\n",
      "Source: <PAD> <SOS> <EOS> jangan untuk air tangan dipakai kubur siraman cuci\n",
      "Target: <PAD> <EOS> koe oeno kaebubusiha mewanuea\n",
      "Predicted: <SOS> <EOS> ini desa kamokula\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> kalau kita harus saling menghormati samasama\n",
      "Target: <PAD> <EOS> ini ane intaidi padapada dopoangkaangkatao\n",
      "Predicted: <SOS> <EOS> ini desa aini nokala\n",
      "\n",
      "Epoch 10/10, Train Loss: 2.3185283088684083\n",
      "Source: <PAD> <SOS> <EOS> kecapi berbentuk seperti perahu\n",
      "Target: <PAD> <EOS> o kusapi peda adhono bhangka\n",
      "Predicted: <SOS> <EOS> desa o peda bhangka\n",
      "\n",
      "Source: <PAD> <SOS> <EOS> dekat keluarga\n",
      "Target: <PAD> <EOS> bhasitie momaho\n",
      "Predicted: <SOS> <EOS> pada desa\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m examples \u001b[38;5;241m=\u001b[39m [test_dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Melakukan prediksi untuk contoh yang dipilih\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Menampilkan source text, target text, dan hasil prediksi\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExamples:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(encoder, decoder, data, vocab)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m token\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m!=\u001b[39m vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<EOS>\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     20\u001b[0m     output, hidden, cell \u001b[38;5;241m=\u001b[39m decoder(token, hidden[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :], cell[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Memastikan ukuran hidden dan cell sesuai\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     topv, topi \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     token \u001b[38;5;241m=\u001b[39m topi\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     23\u001b[0m     predicted_sentence\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main program\n",
    "if __name__ == '__main__':\n",
    "    # Nama file Excel\n",
    "    file_name = 'Corpus_indo.xlsx'\n",
    "    \n",
    "    # Melakukan preprocessing\n",
    "    processed_source_data, processed_target_data = preprocess(file_name)\n",
    "    \n",
    "    # Membangun kosakata\n",
    "    vocab = build_vocab(processed_source_data + processed_target_data)\n",
    "    \n",
    "    # Membuat dataset\n",
    "    dataset = NMTCorpusDataset(processed_source_data, processed_target_data, vocab)\n",
    "    \n",
    "    # Membagi dataset menjadi train dan test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Membuat DataLoader untuk pelatihan dan pengujian\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Batch size 1 untuk prediksi\n",
    "    \n",
    "    # Inisialisasi parameter\n",
    "    input_dim = 128\n",
    "    hidden_dim = 256\n",
    "    vocab_size = len(vocab)\n",
    "    epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Membangun model NMT\n",
    "    encoder, decoder = build_NMT(input_dim, hidden_dim, vocab_size)\n",
    "    \n",
    "    # Melakukan pelatihan\n",
    "    train_NMT(encoder, decoder, train_loader, epochs, learning_rate, vocab)\n",
    "    \n",
    "    # Memilih beberapa contoh dari data pengujian untuk menampilkan hasil\n",
    "    num_examples = 5\n",
    "    indices = random.sample(range(len(test_dataset)), num_examples)\n",
    "    examples = [test_dataset[idx] for idx in indices]\n",
    "    \n",
    "    # Melakukan prediksi untuk contoh yang dipilih\n",
    "    predictions = predict(encoder, decoder, examples, vocab)\n",
    "    \n",
    "    # Menampilkan source text, target text, dan hasil prediksi\n",
    "    print(\"\\nExamples:\")\n",
    "    for example, prediction in zip(examples, predictions):\n",
    "        source_text = [token for token in example[0].numpy().tolist() if token != vocab['<PAD>']]\n",
    "        target_text = [token for token in example[1].numpy().tolist() if token != vocab['<PAD>']]\n",
    "        predicted_text = [token for token in prediction if token != vocab['<EOS>']]\n",
    "        \n",
    "        source_text = ' '.join([key for key, value in vocab.items() if value in source_text])\n",
    "        target_text = ' '.join([key for key, value in vocab.items() if value in target_text])\n",
    "        predicted_text = ' '.join([key for key, value in vocab.items() if value in predicted_text])\n",
    "        \n",
    "        print(f'Source Text: {source_text}')\n",
    "        print(f'Target Text: {target_text}')\n",
    "        print(f'Predicted Text: {predicted_text}\\n')\n",
    "\n",
    "    # Menghitung BLEU score\n",
    "    references = [[sentence[1].numpy().tolist() for sentence in test_dataset]]\n",
    "    bleu_score = calculate_bleu_score(predictions, references)\n",
    "    print(f'BLEU Score: {bleu_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
